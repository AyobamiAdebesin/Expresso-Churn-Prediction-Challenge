{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': (0, 7551), '1': (7551, 15102), '2': (15102, 22653), '3': (22653, 30204), '4': (30204, 37755), '5': (37755, 45306), '6': (45306, 52857), '7': (52857, 60408), '8': (60408, 67959), '9': (67959, 75510), '10': (75510, 83061), '11': (83061, 90612), '12': (90612, 98163), '13': (98163, 105714), '14': (105714, 113265), '15': (113265, 120816), '16': (120816, 128367), '17': (128367, 135918), '18': (135918, 143469), '19': (143469, 151020), '20': (151020, 158571), '21': (158571, 166122), '22': (166122, 173673), '23': (173673, 181224), '24': (181224, 188775), '25': (188775, 196326), '26': (196326, 203877), '27': (203877, 211428), '28': (211428, 218979), '29': (218979, 226530), '30': (226530, 226551)}\n",
      "{'0': (0, 6715), '1': (6715, 13430), '2': (13430, 20145), '3': (20145, 26860), '4': (26860, 33575), '5': (33575, 40290), '6': (40290, 47005), '7': (47005, 53720), '8': (53720, 60435), '9': (60435, 67150), '10': (67150, 73865), '11': (73865, 80580), '12': (80580, 87295), '13': (87295, 94010), '14': (94010, 100725), '15': (100725, 107440), '16': (107440, 114155), '17': (114155, 120870), '18': (120870, 127585), '19': (127585, 134300), '20': (134300, 141015), '21': (141015, 147730), '22': (147730, 154445), '23': (154445, 161160), '24': (161160, 167875), '25': (167875, 174590), '26': (174590, 181305), '27': (181305, 188020), '28': (188020, 194735), '29': (194735, 201450), '30': (201450, 201501)}\n",
      "{'0': (0, 13), '1': (13, 26), '2': (26, 39), '3': (39, 52), '4': (52, 65), '5': (65, 78), '6': (78, 91), '7': (91, 104), '8': (104, 117), '9': (117, 130), '10': (130, 134)}\n",
      "{'0': (0, 13), '1': (13, 26), '2': (26, 39), '3': (39, 52), '4': (52, 65), '5': (65, 78), '6': (78, 91), '7': (91, 104), '8': (104, 117), '9': (117, 121)}\n",
      "{'0': (0, 10), '1': (10, 20), '2': (20, 30), '3': (30, 40), '4': (40, 50), '5': (50, 60), '6': (60, 70), '7': (70, 80), '8': (80, 90), '9': (90, 92)}\n",
      "{'0': (0, 10), '1': (10, 20), '2': (20, 30), '3': (30, 40), '4': (40, 50), '5': (50, 60), '6': (60, 70), '7': (70, 80), '8': (80, 90), '9': (90, 92)}\n",
      "{'0': (0, 1), '1': (1, 2), '2': (2, 3), '3': (3, 4), '4': (4, 5), '5': (5, 6), '6': (6, 7), '7': (7, 8), '8': (8, 9)}\n",
      "{'0': (0, 1), '1': (1, 2), '2': (2, 3), '3': (3, 4), '4': (4, 5), '5': (5, 6), '6': (6, 7), '7': (7, 8), '8': (8, 9)}\n",
      "{'0': (0, 275), '1': (275, 550), '2': (550, 825), '3': (825, 1100), '4': (1100, 1375), '5': (1375, 1650), '6': (1650, 1925), '7': (1925, 2200), '8': (2200, 2475), '9': (2475, 2750), '10': (2750, 2759)}\n",
      "{'0': (0, 308), '1': (308, 616), '2': (616, 924), '3': (924, 1232), '4': (1232, 1540), '5': (1540, 1848), '6': (1848, 2156), '7': (2156, 2464), '8': (2464, 2772), '9': (2772, 3080), '10': (3080, 3084)}\n",
      "{'0': (0, 10), '1': (10, 20), '2': (20, 30), '3': (30, 40), '4': (40, 50), '5': (50, 60), '6': (60, 63)}\n",
      "{'0': (0, 10), '1': (10, 20), '2': (20, 30), '3': (30, 40), '4': (40, 50), '5': (50, 60), '6': (60, 63)}\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os, shutil\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "\n",
    "import base64\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = f\"{name}-{tv}\"\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(\n",
    "        target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    # Regression\n",
    "    return df[result].values.astype(np.float32), df[[target]].values.astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h}:{m:>02}:{s:>05.2f}\"\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred, y, sort=True):\n",
    "    t = pd.DataFrame({'pred': pred, 'y': y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'], inplace=True)\n",
    "    plt.plot(t['y'].tolist(), label='expected')\n",
    "    plt.plot(t['pred'].tolist(), label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean())\n",
    "                          >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "        * (normalized_high - normalized_low) + normalized_low\n",
    "\n",
    "    \n",
    "    \n",
    "# Load the data\n",
    "train = pd.read_csv('Train.csv')\n",
    "\n",
    "test = pd.read_csv('Test.csv')\n",
    "\n",
    "sample_sub = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "variable_def = pd.read_csv('VariableDefinitions.csv')\n",
    "\n",
    "\n",
    "# Set Index\n",
    "train.set_index('user_id', inplace=True)\n",
    "test.set_index('user_id', inplace=True)\n",
    "\n",
    "\n",
    "train['REGION'] = train['REGION'].fillna('DAKAR')\n",
    "test['REGION'] = test['REGION'].fillna('DAKAR')\n",
    "\n",
    "encode_text_dummy(train, name='REGION')\n",
    "encode_text_dummy(test, name='REGION')\n",
    "\n",
    "def enco(data):\n",
    "    if data == 'K > 24 month':\n",
    "        out = 1\n",
    "    else:\n",
    "        out = 0\n",
    "    return out\n",
    "train['TENURE'] = train['TENURE'].apply(enco)\n",
    "test['TENURE'] = test['TENURE'].apply(enco)\n",
    "\n",
    "\n",
    "train['MONTANT'] = train['MONTANT'].fillna(train['MONTANT'].mode()[0])\n",
    "test['MONTANT'] = test['MONTANT'].fillna(test['MONTANT'].mode()[0])\n",
    "\n",
    "def bin_montant(data, min_val, max_val, range_val):\n",
    "    data_copy = data.copy()\n",
    "    range_of_vals = int(max_val+1 - min_val)\n",
    "    batch_value = int(range_of_vals/range_val)\n",
    "    dict_batch = {}\n",
    "    start, stop = 0,0\n",
    "    for i,j in enumerate(range(0,range_of_vals, batch_value)):\n",
    "        stop = start + batch_value\n",
    "        dict_batch[str(i)] = (start,stop)\n",
    "        start = stop\n",
    "        \n",
    "        if stop <= range_of_vals:\n",
    "            pass\n",
    "        else:\n",
    "            start = stop - batch_value\n",
    "            stop = int(max_val) + 1\n",
    "            \n",
    "            dict_batch[str(i)] = (start,stop)\n",
    "    print(dict_batch)       \n",
    "    bin_out = []       \n",
    "    for k in data_copy:\n",
    "        out = [i for i,(start,stop) in zip(dict_batch.keys(), dict_batch.values()) if int(k) in range(start,stop)]\n",
    "        #print(k,out)\n",
    "        bin_out.append(int(out[0]))\n",
    "    return bin_out\n",
    "\n",
    "montant = bin_montant(train['MONTANT'], train['MONTANT'].min(), train['MONTANT'].max(), 30)\n",
    "montant1 = bin_montant(test['MONTANT'], test['MONTANT'].min(), test['MONTANT'].max(), 30)\n",
    "\n",
    "train['MONTANT_BIN'] = montant\n",
    "test['MONTANT_BIN'] = montant1\n",
    "\n",
    "train.drop('MONTANT', inplace=True, axis=1)\n",
    "test.drop('MONTANT', inplace=True, axis=1)\n",
    "\n",
    "def enco(data):\n",
    "    if data == 0:\n",
    "        out = 1\n",
    "    else:\n",
    "        out = 0\n",
    "    return out\n",
    "\n",
    "train['MONTANT_BIN'] = train['MONTANT_BIN'].apply(enco)\n",
    "test['MONTANT_BIN'] = test['MONTANT_BIN'].apply(enco)\n",
    "\n",
    "train['FREQUENCE_RECH'] = train['FREQUENCE_RECH'].fillna(train['FREQUENCE_RECH'].mean())\n",
    "test['FREQUENCE_RECH'] = test['FREQUENCE_RECH'].fillna(test['FREQUENCE_RECH'].mean())\n",
    "train['FREQUENCE_RECH'] = train['FREQUENCE_RECH'].round()\n",
    "test['FREQUENCE_RECH'] = test['FREQUENCE_RECH'].round()\n",
    "\n",
    "def bin_freq_rech(data, min_val, max_val, range_val):\n",
    "    data_copy = data.copy()\n",
    "    range_of_vals = int(max_val+1 - min_val)\n",
    "    batch_value = int(range_of_vals/range_val)\n",
    "    dict_batch = {}\n",
    "    start, stop = 0,0\n",
    "    for i,j in enumerate(range(0,range_of_vals, batch_value)):\n",
    "        stop = start + batch_value\n",
    "        dict_batch[str(i)] = (start,stop)\n",
    "        start = stop\n",
    "        \n",
    "        if stop <= range_of_vals:\n",
    "            pass\n",
    "        else:\n",
    "            start = stop - batch_value\n",
    "            stop = int(max_val) + 1\n",
    "            \n",
    "            dict_batch[str(i)] = (start,stop)\n",
    "    print(dict_batch)       \n",
    "    bin_out = []       \n",
    "    for k in data_copy:\n",
    "        out = [i for i,(start,stop) in zip(dict_batch.keys(), dict_batch.values()) if int(k) in range(start,stop)]\n",
    "        #print(k,out)\n",
    "        bin_out.append(int(out[0]))\n",
    "    return bin_out\n",
    "\n",
    "freq_out = bin_freq_rech(train['FREQUENCE_RECH'], 1, 133, 10)\n",
    "freq_out1 = bin_freq_rech(test['FREQUENCE_RECH'], 1, 120, 9)\n",
    "\n",
    "train['FREQ_BIN'] = freq_out\n",
    "test['FREQ_BIN'] =freq_out1\n",
    "\n",
    "def enco(data):\n",
    "    if data == 0:\n",
    "        out = 1\n",
    "    else:\n",
    "        out =0\n",
    "    return out\n",
    "\n",
    "train['FREQ_BIN'] = train['FREQ_BIN'].apply(enco)\n",
    "test['FREQ_BIN'] = test['FREQ_BIN'].apply(enco)\n",
    "train.drop('FREQUENCE_RECH', inplace=True, axis=1)\n",
    "test.drop('FREQUENCE_RECH', inplace=True, axis=1)\n",
    "\n",
    "\n",
    "train['REVENUE'] = train['REVENUE'].fillna(train['REVENUE'].mean())\n",
    "test['REVENUE'] = test['REVENUE'].fillna(test['REVENUE'].mean())\n",
    "\n",
    "train['REVENUE'] = np.log1p(train['REVENUE'])\n",
    "test['REVENUE'] = np.log1p(test['REVENUE'])\n",
    "\n",
    "train['ARPU_SEGMENT'] = train['ARPU_SEGMENT'].fillna(train['ARPU_SEGMENT'].mean())\n",
    "test['ARPU_SEGMENT'] = test['ARPU_SEGMENT'].fillna(test['ARPU_SEGMENT'].mean())\n",
    "\n",
    "train['ARPU_SEGMENT'] = np.log1p(train['ARPU_SEGMENT'])\n",
    "test['ARPU_SEGMENT'] = np.log1p(test['ARPU_SEGMENT'])\n",
    "\n",
    "train['FREQUENCE'] = train['FREQUENCE'].fillna(train['FREQUENCE'].median())\n",
    "test['FREQUENCE'] = test['FREQUENCE'].fillna(test['FREQUENCE'].median())\n",
    "\n",
    "def bin_freq(data, min_val, max_val, range_val):\n",
    "    data_copy = data.copy()\n",
    "    range_of_vals = int(max_val+1 - min_val)\n",
    "    batch_value = int(range_of_vals/range_val)\n",
    "    dict_batch = {}\n",
    "    start, stop = 0,0\n",
    "    for i,j in enumerate(range(0,range_of_vals, batch_value)):\n",
    "        stop = start + batch_value\n",
    "        dict_batch[str(i)] = (start,stop)\n",
    "        start = stop\n",
    "        \n",
    "        if stop <= range_of_vals:\n",
    "            pass\n",
    "        else:\n",
    "            start = stop - batch_value\n",
    "            stop = int(max_val) + 1\n",
    "            \n",
    "            dict_batch[str(i)] = (start,stop)\n",
    "    print(dict_batch)       \n",
    "    bin_out = []       \n",
    "    for k in data_copy:\n",
    "        out = [i for i,(start,stop) in zip(dict_batch.keys(), dict_batch.values()) if int(k) in range(start,stop)]\n",
    "        #print(k,out)\n",
    "        bin_out.append(int(out[0]))\n",
    "    return bin_out\n",
    "freq1 = bin_freq(train['FREQUENCE'], train['FREQUENCE'].min(), train['FREQUENCE'].max(), 9)\n",
    "freq2 = bin_freq(test['FREQUENCE'], test['FREQUENCE'].min(), test['FREQUENCE'].max(), 9)\n",
    "\n",
    "train['FREQ_BIN'] = freq1\n",
    "test['FREQ_BIN'] = freq2\n",
    "\n",
    "def enco(data):\n",
    "    if data == 0:\n",
    "        out = 1\n",
    "    elif data == 1:\n",
    "        out = 1\n",
    "    else:\n",
    "        out = 0\n",
    "    return out\n",
    "train['FREQ_BIN'] = train['FREQ_BIN'].apply(enco)\n",
    "test['FREQ_BIN'] = test['FREQ_BIN'].apply(enco)\n",
    "train.drop('FREQUENCE', inplace=True, axis =1)\n",
    "test.drop('FREQUENCE', inplace=True, axis =1)\n",
    "\n",
    "train['DATA_VOLUME'] = train['DATA_VOLUME'].fillna(train['DATA_VOLUME'].median())\n",
    "test['DATA_VOLUME'] = test['DATA_VOLUME'].fillna(test['DATA_VOLUME'].median())\n",
    "\n",
    "train['DATA_VOLUME'] = np.log1p(train['DATA_VOLUME'])\n",
    "test['DATA_VOLUME'] = np.log1p(test['DATA_VOLUME'])\n",
    "\n",
    "train['ON_NET'] = train['ON_NET'].fillna(train['ON_NET'].median())\n",
    "test['ON_NET'] = test['ON_NET'].fillna(test['ON_NET'].median())\n",
    "\n",
    "train['ON_NET']= np.log1p(train['ON_NET'])\n",
    "test['ON_NET']= np.log1p(test['ON_NET'])\n",
    "\n",
    "train['ORANGE'] = train['ORANGE'].fillna(train['ORANGE'].median())\n",
    "test['ORANGE'] = test['ORANGE'].fillna(test['ORANGE'].median())\n",
    "\n",
    "\n",
    "train['ORANGE'] = np.log1p(train['ORANGE'])\n",
    "test['ORANGE'] = np.log1p(test['ORANGE'])\n",
    "\n",
    "def bin_orange(data, min_val, max_val, range_val):\n",
    "    data_copy = data.copy()\n",
    "    range_of_vals = int(max_val+1 - min_val)\n",
    "    batch_value = int(range_of_vals/range_val)\n",
    "    dict_batch = {}\n",
    "    start, stop = 0,0\n",
    "    for i,j in enumerate(range(0,range_of_vals, batch_value)):\n",
    "        stop = start + batch_value\n",
    "        dict_batch[str(i)] = (start,stop)\n",
    "        start = stop\n",
    "        \n",
    "        if stop <= range_of_vals:\n",
    "            pass\n",
    "        else:\n",
    "            start = stop - batch_value\n",
    "            stop = int(max_val) + 1\n",
    "            \n",
    "            dict_batch[str(i)] = (start,stop)\n",
    "    print(dict_batch)       \n",
    "    bin_out = []       \n",
    "    for k in data_copy:\n",
    "        out = [i for i,(start,stop) in zip(dict_batch.keys(), dict_batch.values()) if int(k) in range(start,stop)]\n",
    "        #print(k,out)\n",
    "        bin_out.append(int(out[0]))\n",
    "    return bin_out\n",
    "orange = bin_orange(train['ORANGE'], train['ORANGE'].min(), train['ORANGE'].max(), 8)\n",
    "orange1 = bin_orange(test['ORANGE'], test['ORANGE'].min(), test['ORANGE'].max(), 8)\n",
    "train['ORANGE'] = orange\n",
    "test['ORANGE'] = orange1\n",
    "\n",
    "train['TIGO'] = train['TIGO'].fillna(train['TIGO'].mean())\n",
    "test['TIGO'] = test['TIGO'].fillna(test['TIGO'].mean())\n",
    "\n",
    "train['TIGO'] = np.round(train['TIGO'])\n",
    "test['TIGO'] = np.round(test['TIGO'])\n",
    "\n",
    "def bin_tigo(data, min_val, max_val, range_val):\n",
    "    data_copy = data.copy()\n",
    "    range_of_vals = int(max_val+1 - min_val)\n",
    "    batch_value = int(range_of_vals/range_val)\n",
    "    dict_batch = {}\n",
    "    start, stop = 0,0\n",
    "    for i,j in enumerate(range(0,range_of_vals, batch_value)):\n",
    "        stop = start + batch_value\n",
    "        dict_batch[str(i)] = (start,stop)\n",
    "        start = stop\n",
    "        \n",
    "        if stop <= range_of_vals:\n",
    "            pass\n",
    "        else:\n",
    "            start = stop - batch_value\n",
    "            stop = int(max_val) + 1\n",
    "            \n",
    "            dict_batch[str(i)] = (start,stop)\n",
    "    print(dict_batch)       \n",
    "    bin_out = []       \n",
    "    for k in data_copy:\n",
    "        out = [i for i,(start,stop) in zip(dict_batch.keys(), dict_batch.values()) if int(k) in range(start,stop)]\n",
    "        #print(k,out)\n",
    "        bin_out.append(int(out[0]))\n",
    "    return bin_out\n",
    "tigo = bin_tigo(train['TIGO'], train['TIGO'].min(), train['TIGO'].max(), 10)\n",
    "tigo1 = bin_tigo(test['TIGO'], test['TIGO'].min(), test['TIGO'].max(), 10)\n",
    "\n",
    "train['TIGO'] = tigo\n",
    "test['TIGO'] = tigo1\n",
    "\n",
    "def enco(data):\n",
    "    if data == 0:\n",
    "        out = 1\n",
    "    else:\n",
    "        out = 0\n",
    "    return out\n",
    "\n",
    "train['TIGO'] = train['TIGO'].apply(enco)\n",
    "test['TIGO'] = test['TIGO'].apply(enco)\n",
    "\n",
    "to_drop1 = ['ZONE1', 'ZONE2', 'MRG']\n",
    "\n",
    "train.drop(to_drop1, inplace=True, axis=1)\n",
    "test.drop(to_drop1, inplace=True, axis=1)\n",
    "\n",
    "def bin_regu(data, min_val, max_val, range_val):\n",
    "    data_copy = data.copy()\n",
    "    range_of_vals = int(max_val+1 - min_val)\n",
    "    batch_value = int(range_of_vals/range_val)\n",
    "    dict_batch = {}\n",
    "    start, stop = 0,0\n",
    "    for i,j in enumerate(range(0,range_of_vals, batch_value)):\n",
    "        stop = start + batch_value\n",
    "        dict_batch[str(i)] = (start,stop)\n",
    "        start = stop\n",
    "        \n",
    "        if stop <= range_of_vals:\n",
    "            pass\n",
    "        else:\n",
    "            start = stop - batch_value\n",
    "            stop = int(max_val) + 1\n",
    "            \n",
    "            dict_batch[str(i)] = (start,stop)\n",
    "    print(dict_batch)       \n",
    "    bin_out = []       \n",
    "    for k in data_copy:\n",
    "        out = [i for i,(start,stop) in zip(dict_batch.keys(), dict_batch.values()) if int(k) in range(start,stop)]\n",
    "        #print(k,out)\n",
    "        bin_out.append(int(out[0]))\n",
    "    return bin_out\n",
    "regu = bin_regu(train['REGULARITY'], train['REGULARITY'].min(), train['REGULARITY'].max(), 6)\n",
    "regu1 = bin_regu(test['REGULARITY'], test['REGULARITY'].min(), test['REGULARITY'].max(), 6)\n",
    "\n",
    "train['REGULARITY'] = regu\n",
    "test['REGULARITY'] = regu1\n",
    "train.drop('TOP_PACK', axis=1, inplace=True)\n",
    "test.drop('TOP_PACK', axis=1, inplace=True)\n",
    "\n",
    "train['FREQ_TOP_PACK'].fillna(train['FREQ_TOP_PACK'].mean(), inplace=True)\n",
    "test['FREQ_TOP_PACK'].fillna(test['FREQ_TOP_PACK'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Feature Vectors and Target\n",
    "y = train['CHURN'].values\n",
    "train.drop('CHURN', inplace=True, axis=1)\n",
    "X = train.values\n",
    "test_data = test.values\n",
    "\n",
    "# Encode the target\n",
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "import keras\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import TensorBoard, LearningRateScheduler, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(25,), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(1e-3)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(1e-3)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=l2(1e-3)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss=['categorical_crossentropy'], optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold #, 1\n",
      "Epoch 1/100\n",
      "   2/3200 [..............................] - ETA: 9:51 - loss: 2.7341 - accuracy: 0.7650WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.166023). Check your callbacks.\n",
      "3200/3200 [==============================] - 85s 26ms/step - loss: 0.3233 - accuracy: 0.8568 - val_loss: 0.2957 - val_accuracy: 0.8611 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "3200/3200 [==============================] - 84s 26ms/step - loss: 0.2964 - accuracy: 0.8613 - val_loss: 0.2950 - val_accuracy: 0.8617 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "3200/3200 [==============================] - 82s 26ms/step - loss: 0.2953 - accuracy: 0.8615 - val_loss: 0.3001 - val_accuracy: 0.8619 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "3200/3200 [==============================] - 83s 26ms/step - loss: 0.2948 - accuracy: 0.8614 - val_loss: 0.2967 - val_accuracy: 0.8615 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "3200/3200 [==============================] - 102s 32ms/step - loss: 0.2947 - accuracy: 0.8620 - val_loss: 0.2978 - val_accuracy: 0.8622 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "3200/3200 [==============================] - 93s 29ms/step - loss: 0.2947 - accuracy: 0.8624 - val_loss: 0.2982 - val_accuracy: 0.8616 - lr: 0.0010\n",
      "Training fold #, 2\n",
      "Epoch 1/100\n",
      "   2/3200 [..............................] - ETA: 10:16 - loss: 0.2941 - accuracy: 0.8550WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.192784). Check your callbacks.\n",
      "3200/3200 [==============================] - 83s 26ms/step - loss: 0.2975 - accuracy: 0.8605 - val_loss: 0.2939 - val_accuracy: 0.8580 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "3200/3200 [==============================] - 82s 26ms/step - loss: 0.2963 - accuracy: 0.8612 - val_loss: 0.2906 - val_accuracy: 0.8593 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "3200/3200 [==============================] - 82s 26ms/step - loss: 0.2970 - accuracy: 0.8616 - val_loss: 0.2903 - val_accuracy: 0.8615 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "3200/3200 [==============================] - 82s 26ms/step - loss: 0.2955 - accuracy: 0.8619 - val_loss: 0.2932 - val_accuracy: 0.8604 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "3200/3200 [==============================] - 83s 26ms/step - loss: 0.2964 - accuracy: 0.8615 - val_loss: 0.2936 - val_accuracy: 0.8635 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "3200/3200 [==============================] - 82s 26ms/step - loss: 0.2961 - accuracy: 0.8618 - val_loss: 0.2912 - val_accuracy: 0.8606 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "3200/3200 [==============================] - 82s 26ms/step - loss: 0.2973 - accuracy: 0.8617 - val_loss: 0.2911 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Training fold #, 3\n",
      "Epoch 1/100\n",
      "   2/3200 [..............................] - ETA: 11:06 - loss: 0.2605 - accuracy: 0.8650WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.192763). Check your callbacks.\n",
      "3200/3200 [==============================] - 83s 26ms/step - loss: 0.2953 - accuracy: 0.8617 - val_loss: 0.2929 - val_accuracy: 0.8640 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "3200/3200 [==============================] - 81s 25ms/step - loss: 0.2953 - accuracy: 0.8620 - val_loss: 0.2930 - val_accuracy: 0.8638 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "3200/3200 [==============================] - 78s 24ms/step - loss: 0.2953 - accuracy: 0.8614 - val_loss: 0.2920 - val_accuracy: 0.8635 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "3200/3200 [==============================] - 76s 24ms/step - loss: 0.2959 - accuracy: 0.8618 - val_loss: 0.2985 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Epoch 5/100\n",
      " 951/3200 [=======>......................] - ETA: 52s - loss: 0.2976 - accuracy: 0.8617"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "epochs_needed = []\n",
    "score = []\n",
    "fold = 0\n",
    "for train, test in kfold.split(X, y):\n",
    "    start_time = time.time()\n",
    "    fold += 1\n",
    "    X_train = X[train]\n",
    "    y_train = y[train]\n",
    "    X_test = X[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    monitor_1 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=0, mode='auto', restore_best_weights=True)\n",
    "    monitor_2 = ModelCheckpoint(monitor='val_loss', filepath='my_model.h5', save_best_only=True)\n",
    "    monitor_3 = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.1)\n",
    "    monitor_4 = TensorBoard(log_dir = 'my_log_dir', histogram_freq=1)\n",
    "    print('Training fold #,', fold)\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=100, callbacks=[monitor_1, monitor_2, monitor_3, monitor_4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ay = model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['user_id'] = sample_sub['user_id']\n",
    "sub['Churn'] = ay[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>84220893d61733416d3c98ae789dfc091f2cb173</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>118f5f3f99a718ea2dd49c3f93050cec0800e6ed</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>39bc0a79b0ccc1b652372c300344d4defabf6fe6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>e08b899018589f8c6870e6508d70bed2b2226067</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>d7511d775f1251565dac3cd13329aa79125d2bf2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>274f9149c6a9a130a7e82a4c81b9534038550da5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>3fd0f9ac20f0f1a6eaf04be5439a48164fac35f8</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>e4f7a5186364ffff7a026e543a0f7ee894680dc3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>8e6f43168e133e74e406790943974a4a648e01ba</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>f55e262438abbbebe06eade085426371d927efa1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      user_id  Churn\n",
       "500  84220893d61733416d3c98ae789dfc091f2cb173    0.5\n",
       "501  118f5f3f99a718ea2dd49c3f93050cec0800e6ed    0.5\n",
       "502  39bc0a79b0ccc1b652372c300344d4defabf6fe6    0.5\n",
       "503  e08b899018589f8c6870e6508d70bed2b2226067    0.5\n",
       "504  d7511d775f1251565dac3cd13329aa79125d2bf2    0.5\n",
       "..                                        ...    ...\n",
       "596  274f9149c6a9a130a7e82a4c81b9534038550da5    0.5\n",
       "597  3fd0f9ac20f0f1a6eaf04be5439a48164fac35f8    0.5\n",
       "598  e4f7a5186364ffff7a026e543a0f7ee894680dc3    0.5\n",
       "599  8e6f43168e133e74e406790943974a4a648e01ba    0.5\n",
       "600  f55e262438abbbebe06eade085426371d927efa1    0.5\n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.loc[500:600,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "no_1 = pd.read_csv('new.csv')\n",
    "no_2 = pd.read_csv('blending.csv')\n",
    "no_3 = pd.read_csv('cat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pred = np.array(no_1['Churn'])\n",
    "second_pred = np.array(no_2['Churn'])\n",
    "third_pred = np.array(no_3['Churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_all = (first_pred+second_pred+third_pred)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['user_id'] = no_1['user_id']\n",
    "sub['Churn'] = mean_all\n",
    "sub.to_csv('mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
